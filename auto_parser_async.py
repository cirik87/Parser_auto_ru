# auto_parser_async.py (–ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø - –° –û–ë–†–ê–ë–û–¢–ö–û–ô –û–®–ò–ë–û–ö)
"""
–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä auto.ru - –° –ù–ê–î–Å–ñ–ù–û–ô –û–ë–†–ê–ë–û–¢–ö–û–ô –û–®–ò–ë–û–ö
–°–æ–±–∏—Ä–∞–µ—Ç –æ–±—ä—è–≤–ª–µ–Ω–∏—è —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏
"""

import asyncio
import aiohttp
import time
from selenium import webdriver
import pandas as pd
import urllib3
import os
from lxml import html

from pages.listing_page import ListingPage
from config import (
    MAX_PRICE, NUM_THREADS, MAX_PAGES, OUTPUT_FILENAME,
    BASE_URL_TEMPLATE, CHROMEDRIVER_PATH, CHROME_ARGS,
    PAGINATION_DELAY, REQUEST_TIMEOUT
)

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)


class AsyncCarDetailPage:
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π Page Object –¥–ª—è –¥–µ—Ç–∞–ª–µ–π –æ–±—ä—è–≤–ª–µ–Ω–∏—è"""
    
    TITLE = '//h1[@class="CardHead__title"]/text()'
    YEAR = '(//a[@class="Link Link_color_black"]/text())[2]'
    MILEAGE = '(//div[@class="CardInfoSummarySimpleRow__content-IIKcj"]/text())[1]'
    OWNERS = '(//div[@class="CardInfoSummarySimpleRow__content-IIKcj"]/text())[2]'
    CONDITION = '//span[contains(text(), "–ò—Å–ø—Ä–∞–≤–Ω") or contains(text(), "–î–µ—Ñ–æ—Ä–º") or contains(text(), "–ë–∏—Ç—ã–µ") or contains(text(), "–ü–µ—Ä–µ–∫—Ä")]/text()'
    TRANSMISSION = '(//div[@class="CardInfoSummaryComplexRow__cellValue-Hka8p"]/text())[2]'
    ENGINE = '(//div[@class="CardInfoSummaryComplexRow__cellValue-Hka8p"]/text())[1]'
    DATE_POSTED = '//div[contains(@class, "CardHead__creationDate")]/text()'
    VIEWS = '//div[contains(@class, "CardHead__views")]/text()'
    PRICE = '//span[@class="OfferPriceCaption__price"]/text()'
    
    def __init__(self, car_url):
        self.car_url = car_url
        self.tree = None
    
    async def _load_page(self, session, retry=0, max_retries=3):
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏"""
        try:
            async with session.get(
                self.car_url, 
                timeout=aiohttp.ClientTimeout(total=REQUEST_TIMEOUT, connect=3),
                ssl=False,
                allow_redirects=True,
                headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}
            ) as response:
                if response.status == 200:
                    content = await response.read()
                    if content and len(content) > 100:  # ‚úÖ –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –ø—É—Å—Ç–æ–π –ª–∏ –æ—Ç–≤–µ—Ç
                        self.tree = html.fromstring(content)
                        return True
                elif response.status == 429 and retry < max_retries:
                    # ‚úÖ Too Many Requests - –ø–æ–≤—Ç–æ—Ä–∏—Ç—å –ø–æ–∑–∂–µ
                    await asyncio.sleep(0.5)
                    return await self._load_page(session, retry + 1, max_retries)
        except asyncio.TimeoutError:
            if retry < max_retries:
                await asyncio.sleep(0.2)
                return await self._load_page(session, retry + 1, max_retries)
        except Exception as e:
            if retry < max_retries:
                await asyncio.sleep(0.1)
                return await self._load_page(session, retry + 1, max_retries)
        return False
    
    def _get_text(self, xpath, index=0):
        """–ü–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—Å—Ç –ø–æ XPath"""
        if self.tree is None:
            return "N/A"
        try:
            result = self.tree.xpath(xpath)
            if result and len(result) > index:
                text = str(result[index]).strip()
                return text if text else "N/A"
        except:
            pass
        return "N/A"
    
    def get_date_posted(self):
        """–ü–æ–ª—É—á–∏—Ç—å –¥–∞—Ç—É –æ–±—ä—è–≤–ª–µ–Ω–∏—è"""
        try:
            if self.tree is None:
                return "N/A"
            
            date_patterns = [
                '//div[contains(@class, "CardHead__creationDate")]/text()',
                '//div[@class="CardHead__infoItem CardHead__creationDate"]/text()',
            ]
            
            for xpath in date_patterns:
                try:
                    result = self.tree.xpath(xpath)
                    if result:
                        for r in result:
                            text = str(r).strip()
                            if text and len(text) > 3:
                                return text
                except:
                    pass
            
            return "N/A"
        except:
            return "N/A"
    
    def get_views(self):
        """–ü–æ–ª—É—á–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤"""
        try:
            if self.tree is None:
                return "0"
            
            view_patterns = [
                '//div[contains(@class, "CardHead__views")]/text()',
                '//div[@class="CardHead__infoItem CardHead__views"]/text()',
            ]
            
            for xpath in view_patterns:
                try:
                    result = self.tree.xpath(xpath)
                    if result:
                        for r in result:
                            text = str(r).strip()
                            numbers = ''.join(filter(str.isdigit, text))
                            if numbers:
                                return numbers
                except:
                    pass
            
            return "0"
        except:
            return "0"
    
    def get_condition(self):
        """–ü–æ–ª—É—á–∏—Ç—å —Å–æ—Å—Ç–æ—è–Ω–∏–µ"""
        try:
            if self.tree is None:
                return "N/A"
            
            conditions = [
                '//span[contains(text(), "–ò—Å–ø—Ä–∞–≤–Ω–æ–µ")]/text()',
                '//span[contains(text(), "–î–µ—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ")]/text()',
                '//span[contains(text(), "–ë–∏—Ç—ã–µ –æ–∫–Ω–∞")]/text()',
                '//span[contains(text(), "–ü–µ—Ä–µ–∫—Ä–∞—à–µ–Ω–æ")]/text()',
                '//span[contains(., "–ò—Å–ø—Ä–∞–≤–Ω") or contains(., "–î–µ—Ñ–æ—Ä–º") or contains(., "–ë–∏—Ç—ã–µ") or contains(., "–ü–µ—Ä–µ–∫—Ä")]/text()',
            ]
            
            for xpath in conditions:
                try:
                    result = self.tree.xpath(xpath)
                    if result:
                        text = str(result[0]).strip()
                        if text and text != "N/A":
                            return text
                except:
                    pass
            
            return "N/A"
        except:
            return "N/A"
    
    def get_price(self):
        """–ü–æ–ª—É—á–∏—Ç—å —Ü–µ–Ω—É"""
        try:
            price_text = self._get_text(self.PRICE)
            if price_text != "N/A":
                numbers = ''.join(filter(str.isdigit, price_text))
                if numbers:
                    return int(numbers)
        except:
            pass
        return 0
    
    def get_car_data(self):
        """–ü–æ–ª—É—á–∏—Ç—å –≤—Å–µ –¥–∞–Ω–Ω—ã–µ"""
        return {
            '–ú–∞—Ä–∫–∞': self._get_text(self.TITLE),
            '–ì–æ–¥ –≤—ã–ø—É—Å–∫–∞': self._get_text(self.YEAR),
            '–ü—Ä–æ–±–µ–≥': self._get_text(self.MILEAGE),
            '–í–ª–∞–¥–µ–ª—å—Ü—ã': self._get_text(self.OWNERS),
            '–°–æ—Å—Ç–æ—è–Ω–∏–µ': self.get_condition(),
            '–ö–æ—Ä–æ–±–∫–∞': self._get_text(self.TRANSMISSION),
            '–î–≤–∏–≥–∞—Ç–µ–ª—å': self._get_text(self.ENGINE),
            '–î–∞—Ç–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏—è': self.get_date_posted(),
            '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤': self.get_views(),
            '–¶–µ–Ω–∞': self.get_price(),
            'URL': self.car_url
        }


class AsyncAutoRuParser:
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä auto.ru - –í–°–ï –æ–±—ä—è–≤–ª–µ–Ω–∏—è"""
    
    def __init__(self, max_price=MAX_PRICE, concurrent_requests=NUM_THREADS * 2):
        self.base_url = BASE_URL_TEMPLATE
        self.max_price = max_price
        self.cars = []
        self.driver = None
        self.concurrent_requests = concurrent_requests
        self.stats = {'processed': 0, 'errors': 0, 'skipped': 0}
    
    def setup_driver(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å Selenium"""
        options = webdriver.ChromeOptions()
        for arg in CHROME_ARGS:
            options.add_argument(arg)
        
        try:
            if os.path.exists(CHROMEDRIVER_PATH):
                self.driver = webdriver.Chrome(CHROMEDRIVER_PATH, options=options)
            else:
                self.driver = webdriver.Chrome(options=options)
            print("‚úÖ ChromeDriver –∑–∞–≥—Ä—É–∂–µ–Ω")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
            raise
    
    def collect_all_links(self, max_pages=MAX_PAGES):
        """–°–æ–±—Ä–∞—Ç—å –í–°–ï —Å—Å—ã–ª–∫–∏"""
        listing_page = ListingPage(self.driver, self.base_url)
        
        print(f"üîç –û–ø—Ä–µ–¥–µ–ª—è—é –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü...")
        total_pages = listing_page.get_total_pages()
        
        if max_pages:
            total_pages = min(total_pages, max_pages)
        
        print(f"üìä –ë—É–¥—É—Ç —Å–æ–±—Ä–∞–Ω—ã —Å—Å—ã–ª–∫–∏ —Å–æ –í–°–ï–• {total_pages} —Å—Ç—Ä–∞–Ω–∏—Ü\n")
        
        all_links = []
        failed_pages = []
        
        for page in range(1, total_pages + 1):
            try:
                print(f"üìÑ –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page}/{total_pages}...", end=' ', flush=True)
                listing_page.open_page(page)
                
                links = listing_page.get_car_links()
                all_links.extend(links)
                print(f"‚úÖ {len(links)} —Å—Å—ã–ª–æ–∫ (–≤—Å–µ–≥–æ: {len(all_links)})")
                
                if PAGINATION_DELAY > 0:
                    time.sleep(PAGINATION_DELAY)
            
            except Exception as e:
                print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞: {e}")
                failed_pages.append(page)
        
        if failed_pages:
            print(f"\n‚ö†Ô∏è  –û—à–∏–±–∫–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ö: {failed_pages}")
        
        return all_links
    
    async def parse_car_async(self, session, car_url, semaphore):
        """–ü–∞—Ä—Å–∏—Ç—å –æ–±—ä—è–≤–ª–µ–Ω–∏–µ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ"""
        async with semaphore:
            try:
                detail_page = AsyncCarDetailPage(car_url)
                if not await detail_page._load_page(session):
                    self.stats['errors'] += 1
                    return
                
                car_data = detail_page.get_car_data()
                
                # ‚úÖ –ü—Ä–∏–Ω–∏–º–∞–µ–º –í–°–ï –æ–±—ä—è–≤–ª–µ–Ω–∏—è
                self.cars.append(car_data)
                self.stats['processed'] += 1
                
                if self.stats['processed'] % 100 == 0:
                    success_rate = (self.stats['processed'] / (self.stats['processed'] + self.stats['errors'])) * 100 if (self.stats['processed'] + self.stats['errors']) > 0 else 0
                    print(f"  ‚ö° –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {self.stats['processed']} | –£—Å–ø–µ—Ö: {success_rate:.1f}%")
            
            except Exception:
                self.stats['errors'] += 1
    
    async def parse_all_async(self, all_links):
        """–ü–∞—Ä—Å–∏—Ç—å –≤—Å–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ"""
        print(f"\n{'='*60}")
        print(f"üìä –í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ —Å—Å—ã–ª–æ–∫: {len(all_links)}")
        print(f"‚ö° –û–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤: {self.concurrent_requests}")
        print(f"{'='*60}\n")
        
        start_parsing = time.time()
        
        semaphore = asyncio.Semaphore(self.concurrent_requests)
        
        # ‚úÖ –£–õ–£–ß–®–ï–ù–û: –õ—É—á—à–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∫–æ–Ω–Ω–µ–∫—Ç–æ—Ä–∞
        connector = aiohttp.TCPConnector(
            limit=self.concurrent_requests,
            limit_per_host=5,  # ‚úÖ –°–Ω–∏–∂–∞–µ–º –Ω–∞–≥—Ä—É–∑–∫—É –Ω–∞ —Ö–æ—Å—Ç
            ttl_dns_cache=300,
            enable_cleanup_closed=True,
            keepalive_timeout=30,
            force_close=False,
            ssl=False
        )
        
        timeout = aiohttp.ClientTimeout(total=REQUEST_TIMEOUT, connect=3, sock_read=3)
        
        async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:
            tasks = [
                self.parse_car_async(session, url, semaphore)
                for url in all_links
            ]
            
            await asyncio.gather(*tasks, return_exceptions=True)
        
        elapsed_parsing = time.time() - start_parsing
        print(f"\n‚ö° –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –∑–∞–Ω—è–ª: {elapsed_parsing:.1f} —Å–µ–∫")
        return elapsed_parsing
    
    def parse_all_pages(self, max_pages=MAX_PAGES):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –ø–∞—Ä—Å–∏–Ω–≥–∞"""
        self.setup_driver()
        
        try:
            all_links = self.collect_all_links(max_pages)
            asyncio.run(self.parse_all_async(all_links))
        
        except KeyboardInterrupt:
            print("\n\n‚ö†Ô∏è  –ü–∞—Ä—Å–∏–Ω–≥ –ø—Ä–µ—Ä–≤–∞–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        
        finally:
            self.driver.quit()
            print("üî¥ Browser –∑–∞–∫—Ä—ã—Ç")
        
        return self.cars
    
    def save_to_excel(self, filename=OUTPUT_FILENAME):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –í–°–ï –¥–∞–Ω–Ω—ã–µ"""
        if not self.cars:
            print("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
            return
        
        start_save = time.time()
        print(f"\n‚è≥ –°–æ—Ö—Ä–∞–Ω—è—é {len(self.cars)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π...")
        
        df = pd.DataFrame(self.cars)
        
        # ‚úÖ –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        try:
            df['–î–∞—Ç–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏—è'] = pd.to_datetime(df['–î–∞—Ç–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏—è'], 
                                                   format='%d %B %Y', 
                                                   errors='coerce')
            df = df.sort_values('–î–∞—Ç–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏—è', ascending=False, na_position='last')
            df['–î–∞—Ç–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏—è'] = df['–î–∞—Ç–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏—è'].dt.strftime('%d.%m.%Y')
        except:
            pass
        
        # ‚úÖ –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –ø—Ä–æ—Å–º–æ—Ç—Ä—ã –≤ —á–∏—Å–ª–∞
        try:
            df['–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤'] = pd.to_numeric(df['–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤'], errors='coerce').fillna(0).astype(int)
        except:
            pass
        
        df = df.drop('URL', axis=1)
        df.to_excel(filename, index=False, engine='openpyxl')
        
        elapsed_save = time.time() - start_save
        
        print(f"\n‚úÖ –î–ê–ù–ù–´–ï –°–û–•–†–ê–ù–ï–ù–´!")
        print(f"üìä –ó–∞–ø–∏—Å–µ–π –≤ —Ñ–∞–π–ª–µ: {len(df)}")
        print(f"üìà –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {self.stats['processed']}")
        print(f"‚ùå –û—à–∏–±–æ–∫: {self.stats['errors']}")
        
        success_rate = (self.stats['processed'] / (self.stats['processed'] + self.stats['errors'])) * 100 if (self.stats['processed'] + self.stats['errors']) > 0 else 0
        print(f"üìä –ü—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—Ö–∞: {success_rate:.1f}%")
        
        if len(df) > 0:
            print(f"\nüí∞ –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ü–µ–Ω–∞–º:")
            print(f"   –°—Ä–µ–¥–Ω—è—è —Ü–µ–Ω–∞: {df['–¶–µ–Ω–∞'].mean():,.0f} —Ä—É–±")
            print(f"   –ú–∏–Ω–∏–º—É–º: {df['–¶–µ–Ω–∞'].min():,.0f} —Ä—É–±")
            print(f"   –ú–∞–∫—Å–∏–º—É–º: {df['–¶–µ–Ω–∞'].max():,.0f} —Ä—É–±")
            print(f"   –û–±—ä—è–≤–ª–µ–Ω–∏–π –±–µ–∑ —Ü–µ–Ω—ã: {(df['–¶–µ–Ω–∞'] == 0).sum()}")


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    total_start = time.time()
    
    parser = AsyncAutoRuParser(
        max_price=MAX_PRICE, 
        concurrent_requests=NUM_THREADS * 2  # ‚úÖ –£–º–µ–Ω—å—à–µ–Ω–æ —Å 3x –¥–æ 2x
    )
    
    print("\n" + "="*60)
    print("üöó –ê–°–ò–ù–•–†–û–ù–ù–´–ô –ü–ê–†–°–ï–† AUTO.RU")
    print("   –ù–ê–î–Å–ñ–ù–ê–Ø –í–ï–†–°–ò–Ø –° –û–ë–†–ê–ë–û–¢–ö–û–ô –û–®–ò–ë–û–ö")
    print("="*60)
    print(f"üí∞ –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Ü–µ–Ω–∞: {MAX_PRICE:,} —Ä—É–±")
    print(f"‚ö° –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤: {parser.concurrent_requests}")
    print("="*60)
    
    parser.parse_all_pages(max_pages=MAX_PAGES)
    parser.save_to_excel(OUTPUT_FILENAME)
    
    total_elapsed = time.time() - total_start
    
    print("\n" + "="*60)
    print(f"‚úÖ –ü–ê–†–°–ò–ù–ì –£–°–ü–ï–®–ù–û –ó–ê–í–ï–†–®–Å–ù!")
    print("="*60)
    print(f"‚è±Ô∏è  –û–±—â–µ–µ –≤—Ä–µ–º—è: {total_elapsed:.1f} —Å–µ–∫")
    print(f"üìä –í—Å–µ–≥–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π –≤ —Ñ–∞–π–ª–µ: {len(parser.cars)}")
    
    if total_elapsed > 0 and len(parser.cars) > 0:
        speed = len(parser.cars) / total_elapsed
        print(f"‚ö° –°–∫–æ—Ä–æ—Å—Ç—å –ø–∞—Ä—Å–∏–Ω–≥–∞: ~{speed:.1f} –æ–±—ä—è–≤–ª/—Å–µ–∫ üöÄ")
    
    print("="*60)
    print(f"üìÅ –§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {OUTPUT_FILENAME}\n")


if __name__ == "__main__":
    main()
